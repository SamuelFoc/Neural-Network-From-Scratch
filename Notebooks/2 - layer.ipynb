{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Neural Network Layer\n",
    "\n",
    "Now that we understand what the `Perceptron` is, let's delve into creating a more complex structure than just a single perceptron or a perceptron chain. Our goal is to construct a perceptron layer, envisioning it as a column of perceptrons.\n",
    "\n",
    "The data flow in this structure follows these steps:\n",
    "\n",
    "- All the data is fed into every perceptron in the layer.\n",
    "- Each perceptron produces a single output value.\n",
    "- The outputs of all perceptrons are combined into a single column vector output.\n",
    "- The output vector from one layer can serve as the input vector for the next layer.\n",
    "\n",
    "### Theory\n",
    "\n",
    "To build a layer of perceptrons, we already know that each perceptron must have a precise number of weights, one bias, and an activation function. Let's assume that all perceptrons in the layer share the same activation function, differing only in their input weights and biases. For instance, in a layer with three perceptrons and three inputs, we should have three weights and one bias for each perceptron (one weight for each input and one bias for each perceptron). Adding them up, we should have nine weights (3 perceptrons Ã— 3 inputs) and 3 biases.\n",
    "\n",
    "To manage these weights and biases efficiently, we can index them by associating two indices with each weight: one for the input it belongs to and the second for the perceptron. For example, the weight for the first perceptron and its second input would be indexed as $w_{12}$. These indices can be structured like matrix notation, leading us to represent all the weights of the layer using a weights matrix $\\mathbb{W}$.\n",
    "\n",
    "$$\n",
    "\\mathbb{W} = \\begin{bmatrix}\n",
    "    w_{11} & w_{12} & \\dots & w_{1m} \\\\\n",
    "    w_{21} & w_{22} & \\dots & w_{2m} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    w_{n1} & w_{n2} & \\dots & w_{nm} \\\\\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "where $m$ is the number of inputs to the layer, and $n$ is the number of perceptrons in the layer.\n",
    "\n",
    "Now, let's consider biases and organize them into a column vector $\\vec{b}$:\n",
    "\n",
    "$$\n",
    "\\vec{b} = \\begin{bmatrix}\n",
    "    b_{1} \\\\\n",
    "    b_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    b_{n} \\\\\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "where $n$ is the number of perceptrons in the layer. Thus, we can describe the layer using three mathematical terms: $\\mathbb{W}, \\vec{b}, \\sigma$, where $\\sigma$ denotes the activation function. Now, let's proceed to create a layer.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "It's time to implement the aforementioned theory. Initially, we need to initialize the weights and biases, and for simplicity, we'll use random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, m_inputs:int, n_perceptrons:int):\n",
    "        \"\"\"\n",
    "        Initialize a layer with random weights and biases.\n",
    "\n",
    "        Parameters:\n",
    "        - m_inputs: Number of input features.\n",
    "        - n_perceptrons: Number of perceptrons (neurons) in the layer.\n",
    "        - activation: String representing the activation function.\n",
    "                      Default is 'relu'.\n",
    "        \"\"\"\n",
    "        self.m = m_inputs\n",
    "        self.n = n_perceptrons                           \n",
    "        self.weights_matrix = np.random.rand(self.m, self.n)\n",
    "        self.biases_vector = np.zeros(self.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the weights matrix are ((3, 3))\n",
      "Weight matrix of the layer: \n",
      "[[0.93052064 0.19921122 0.72346368]\n",
      " [0.43208035 0.21682592 0.76378149]\n",
      " [0.89198978 0.57299042 0.89197121]]\n",
      "\n",
      "Dimensions of the biases vector are ((3,))\n",
      "Biases vector of the layer: \n",
      "[0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "my_first_layer = Layer(m_inputs=3, n_perceptrons=3)\n",
    "print(f\"Dimensions of the weights matrix are ({my_first_layer.weights_matrix.shape})\")\n",
    "print(f\"Weight matrix of the layer: \\n{my_first_layer.weights_matrix}\")\n",
    "\n",
    "print(f\"\\nDimensions of the biases vector are ({my_first_layer.biases_vector.shape})\")\n",
    "print(f\"Biases vector of the layer: \\n{my_first_layer.biases_vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the weight matrix and biases vector for the layer, let's focus on the activation function and a method that allows us to pass inputs through the layer, obtaining all the activated results. Let's tackle these steps one by one, starting with the activation function.\n",
    "\n",
    "#### Activation\n",
    "\n",
    "The activation function, denoted as $\\sigma$, plays a crucial role in determining the output of each perceptron in the layer. Common activation functions include the sigmoid function, hyperbolic tangent (tanh), or rectified linear unit (ReLU). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, m_inputs: int, n_perceptrons: int, activation: str = 'relu'):\n",
    "        self.m = m_inputs\n",
    "        self.n = n_perceptrons                           \n",
    "        self.weights_matrix = np.random.rand(self.m, self.n)\n",
    "        self.biases_vector = np.zeros(self.n)\n",
    "        self._set_activation(activation)\n",
    "\n",
    "    \n",
    "    def _set_activation(self, activation: str):\n",
    "        \"\"\"\n",
    "        Set the activation function based on the input string.\n",
    "\n",
    "        Parameters:\n",
    "        - activation: String representing the activation function.\n",
    "        \"\"\"\n",
    "        if activation == 'relu':\n",
    "            self.activation = self.relu\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = self.sigmoid\n",
    "        elif activation == 'softmax':\n",
    "            self.activation = self.softmax\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = self.tanh\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def relu(self, x):\n",
    "        \"\"\"\n",
    "        Rectified Linear Unit (ReLU) activation function.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input.\n",
    "\n",
    "        Returns:\n",
    "        - Output after applying ReLU.\n",
    "        \"\"\"\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Sigmoid activation function.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input.\n",
    "\n",
    "        Returns:\n",
    "        - Output after applying sigmoid.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Softmax activation function.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input.\n",
    "\n",
    "        Returns:\n",
    "        - Output after applying softmax.\n",
    "        \"\"\"\n",
    "        exp_values = np.exp(x - np.max(x))\n",
    "        return exp_values / exp_values.sum(axis=0, keepdims=True)\n",
    "\n",
    "    def tanh(self, x):\n",
    "        \"\"\"\n",
    "        Hyperbolic Tangent (tanh) activation function.\n",
    "\n",
    "        Parameters:\n",
    "        - x: Input.\n",
    "\n",
    "        Returns:\n",
    "        - Output after applying tanh.\n",
    "        \"\"\"\n",
    "        return np.tanh(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Computations\n",
    "\n",
    "Now that we have some activation functions in the layer, let's create a method that can propagate the inputs through the layer. We will call this method `forward`, but don't worry about the name for now; it will become clear later why we chose this name.\n",
    "\n",
    "So, how should we compute the output of this layer? We already know that we can compute the output of one perceptron as the dot product between the inputs and weights, add bias, and activate using the activation function. What changes with more perceptrons now? We can compute the individual dot products and get the results, and it would be totally correct. However, we can make a clever calculation, realizing that the individual dot products will give us the output vector of individual perceptron outputs.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "y_1 &= \\sigma(\\sum_{i=1}^m w_{1i}x_i + b_1)\\\\\n",
    "y_2 &= \\sigma(\\sum_{i=1}^m w_{2i}x_i + b_2)\\\\\n",
    "& \\vdots \\\\\n",
    "y_n &= \\sigma(\\sum_{i=1}^m w_{ni}x_i + b_n)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $y_1, y_2,..., y_n$ are the outputs from individual perceptrons. We could compute the output now and be totally correct, but let's take a little dive into matrix computation again. Imagine that we have a matrix $\\mathbb{W}$ and the vector $\\vec{x}$. Let's see how the product of these two objects would look like.\n",
    "\n",
    "$$\n",
    "\\mathbb{W} \\cdot \\vec{x}^T = \\begin{bmatrix}\n",
    "    w_{11} & w_{12} & \\dots & w_{1m} \\\\\n",
    "    w_{21} & w_{22} & \\dots & w_{2m} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    w_{n1} & w_{n2} & \\dots & w_{nm} \\\\\n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "    x_{1} \\\\\n",
    "    x_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    x_{m} \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\sum_{i=1}^{m} w_{1i}x_i \\\\\n",
    "    \\sum_{i=1}^{m} w_{2i}x_i \\\\\n",
    "    \\vdots \\\\\n",
    "    \\sum_{i=1}^{m} w_{ni}x_i \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This result looks quite similar to the result that we got from individual perceptrons. If we just added the biases vector $\\vec{b}$ to the matrix dot product, we would be even closer to the previous result.\n",
    "\n",
    "$$\n",
    "\\mathbb{W} \\cdot \\vec{x}^T + \\vec{b} = \\begin{bmatrix}\n",
    "    w_{11} & w_{12} & \\dots & w_{1m} \\\\\n",
    "    w_{21} & w_{22} & \\dots & w_{2m} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    w_{n1} & w_{n2} & \\dots & w_{nm} \\\\\n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "    x_{1} \\\\\n",
    "    x_{2} \\\\\n",
    "    \\vdots \\\\\n",
    "    x_{n} \\\\\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "    b_1 \\\\\n",
    "    b_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    b_n \\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\sum_{i=1}^{m} w_{1i}x_i + b_1 \\\\\n",
    "    \\sum_{i=1}^{m} w_{2i}x_i + b_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    \\sum_{i=1}^{m} w_{ni}x_i + b_n \\\\\n",
    "\\end{bmatrix} = \\vec{a}\n",
    "$$\n",
    "\n",
    "We are almost there. Now we just need to activate all the components from the result. We can do it by using a function that consumes a vector, applies it to all its components, and then returns a new activated vector.\n",
    "\n",
    "$$\n",
    "\\sigma(\\vec{a}) = \\sigma(\\mathbb{W}\\cdot\\vec{x}^T + \\vec{b}) = \\begin{bmatrix}\n",
    "    \\sigma(\\sum_{i=1}^{m} w_{1i}x_i + b_1)\\\\\n",
    "    \\sigma(\\sum_{i=1}^{m} w_{2i}x_i + b_2)\\\\\n",
    "    \\vdots \\\\\n",
    "    \\sigma(\\sum_{i=1}^{m} w_{ni}x_i + b_n)\\\\\n",
    "\\end{bmatrix} = \\vec{y}\n",
    "$$\n",
    "\n",
    "Now we can see that instead of looping over the individual perceptrons, we can just define a matrix consisting of all the weights from the layer and compute the dot product with the inputs, add a biases vector, and then apply the activation to all components of the resulting vector. Let's implement this now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, m_inputs: int, n_perceptrons: int, activation: str = 'relu'):\n",
    "        self.m = m_inputs\n",
    "        self.n = n_perceptrons                           \n",
    "        self.weights_matrix = np.random.rand(self.m, self.n)\n",
    "        self.biases_vector = np.zeros(self.n)\n",
    "        self._set_activation(activation)\n",
    "\n",
    "    \n",
    "    def _set_activation(self, activation: str):\n",
    "        if activation == 'relu':\n",
    "            self.activation = self.relu\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = self.sigmoid\n",
    "        elif activation == 'softmax':\n",
    "            self.activation = self.softmax\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = self.tanh\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_values = np.exp(x - np.max(x))\n",
    "        return exp_values / exp_values.sum(axis=0, keepdims=True)\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "        \n",
    "    def forward(self, inputs:list):\n",
    "        \"\"\"\n",
    "        Perform forward pass through the layer.\n",
    "\n",
    "        Parameters:\n",
    "        - inputs: List of input values.\n",
    "\n",
    "        Returns:\n",
    "        - activation: Output after applying activation function.\n",
    "        \"\"\"\n",
    "        # Calculate the weighted sum of inputs and add biases\n",
    "        argument = np.dot(self.weights_matrix.T, inputs) + self.biases_vector\n",
    "\n",
    "        # Apply the activation function\n",
    "        activation = self.activation(argument)\n",
    "        return activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [0.70028256 0.18720134 0.80807865]\n",
      " y = [1.2176225  1.41055608 0.34167211]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(3)\n",
    "\n",
    "layer = Layer(m_inputs=3, n_perceptrons=3, activation=\"relu\")\n",
    "y = layer.forward(x)\n",
    "\n",
    "print(f\"x = {x}\\n y = {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Real Neural Network\n",
    "\n",
    "Now that we can create a real neural network by chaining layers, passing the output of the first layer as input into the second, and so on, we find ourselves with just random values. Unfortunately, we can't make any predictions using a neural network in this state. We need to figure out how to adjust the weights and biases of individual layers in the network so we can make predictions based on training data. This process will be discussed in the next chapter.\n",
    "\n",
    "**NOTE:** In the implementation, we used $\\mathbb{W}^T\\cdot\\vec{x}$ instead of $\\mathbb{W}\\cdot\\vec{x}^T$ because numpy can't transpose a 1D vector. The result of this operation is just a transposed vector $\\vec{y}^T$, but for our purposes, it is exactly the same as $\\vec{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [0.62581241 0.15579656 0.74573661 0.72584999 0.25279547 0.42913702\n",
      " 0.16370503 0.64355034]\n",
      "\n",
      "y_1 = [2.02617686 1.81485014 1.24673764 1.89551142 1.97027812 1.42067571\n",
      " 1.41836614 1.28796219]\n",
      "\n",
      "y_2 = [8.82083583 7.3789929  7.22495954 8.28830517 6.14173435 7.00702324\n",
      " 6.08710616 6.9519832  5.74782273 8.22040577 4.79499482 7.06348432\n",
      " 5.83752702 5.99333522 6.13574526 7.12622832]\n",
      "\n",
      "y_3 = [47.91753259 57.17937117 58.0718805  48.9766543  53.53069753 48.96906559\n",
      " 65.40277461 49.64824271 54.0331496  43.84550337 62.73920104 73.28308745\n",
      " 73.73312402 56.51938494 61.6087385  46.76370827]\n",
      "\n",
      "y_4 = [564.19262031 405.32983071 578.77282214 462.94881905 456.94097017\n",
      " 528.97578901 343.99363559 547.55514324 467.81780315 429.01843353\n",
      " 263.65203715 423.55076047 478.94673745 490.72058938 467.204378\n",
      " 513.79553742]\n",
      "\n",
      "y_5 = [4165.99325259 4022.00690757 4730.65211406 3183.88019892 3883.338997\n",
      " 2916.77604951 3895.66718585 4258.0535298 ]\n",
      "\n",
      "Result = [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(8)\n",
    "\n",
    "layer_1 = Layer(m_inputs=8, n_perceptrons=8, activation=\"relu\")\n",
    "layer_2 = Layer(m_inputs=8, n_perceptrons=16, activation=\"relu\")\n",
    "layer_3 = Layer(m_inputs=16, n_perceptrons=16, activation=\"relu\")\n",
    "layer_4 = Layer(m_inputs=16, n_perceptrons=16, activation=\"relu\")\n",
    "layer_5 = Layer(m_inputs=16, n_perceptrons=8, activation=\"relu\")\n",
    "layer_6 = Layer(m_inputs=8, n_perceptrons=3, activation=\"tanh\")\n",
    "\n",
    "y_1 = layer_1.forward(x)\n",
    "y_2 = layer_2.forward(y_1)\n",
    "y_3 = layer_3.forward(y_2)\n",
    "y_4 = layer_4.forward(y_3)\n",
    "y_5 = layer_5.forward(y_4)\n",
    "result = layer_6.forward(y_5)\n",
    "\n",
    "print(f\"x = {x}\\n\")\n",
    "print(f\"y_1 = {y_1}\\n\")\n",
    "print(f\"y_2 = {y_2}\\n\")\n",
    "print(f\"y_3 = {y_3}\\n\")\n",
    "print(f\"y_4 = {y_4}\\n\")\n",
    "print(f\"y_5 = {y_5}\\n\")\n",
    "print(f\"Result = {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
